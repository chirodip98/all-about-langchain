{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a87975-584f-47f6-8894-16dedb6b5954",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd89563-55f1-4603-aabf-e1af7030b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is the capital of India ?')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_1 = PromptTemplate.from_template(\"What is the capital of {country} ?\")\n",
    "PROMPT_1.invoke('India')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cbd7c-17fb-4668-bebd-237dfa43636a",
   "metadata": {},
   "source": [
    "Method 2. Creating a PromptTemplate object and a prompt all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29157c35-3285-411f-9a08-c5ba1d91f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define template\n",
    "template = \"What is the capital of {country}?\"\n",
    "\n",
    "# Create a prompt template with `PromptTemplate` object\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880add-0ed3-455e-a98f-b739fffd1201",
   "metadata": {},
   "source": [
    "Using partial_variables , you can partially apply functions. This is particularly useful when there are common variables to be shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6ffe6f-f471-454b-8e43-0795b56dacb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': 'United States of America'}, template='What are the capitals of {country1} and {country2}, respectively?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define template\n",
    "template = \"What are the capitals of {country1} and {country2}, respectively?\"\n",
    "\n",
    "# Create a prompt template with `PromptTemplate` object\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country1\"],\n",
    "    partial_variables={\n",
    "        \"country2\": \"United States of America\"  # Pass `partial_variables` in dictionary form\n",
    "    },\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ca6875-b191-4056-a524-782fc9cbbbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the capitals of South Korea and United States of America, respectively?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(country1=\"South Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b92828-2aa5-49ea-b16d-dba6ccdb7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define function to return the current date\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%B %d\")\n",
    "    \n",
    "prompt = PromptTemplate(\n",
    "    template=\"Today's date is {today}. Please list {n} celebrities whose birthday is today. Please specify their date of birth.\",\n",
    "    input_variables=[\"n\"],\n",
    "    partial_variables={\n",
    "        \"today\": get_today  # Pass `partial_variables` in dictionary form\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe718db-58cc-4df1-81a6-690dcb622871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is February 28. Please list 3 celebrities whose birthday is today. Please specify their date of birth.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create prompt\n",
    "prompt.format(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b486f-7fd6-4704-84b6-484f68bd79d0",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "    ChatPromptTemplate can be used to include a conversation history as a prompt.\n",
    "    \n",
    "    Messages are structured as tuples in the format (role , message ) and are created as a list.\n",
    "    \n",
    "    role\n",
    "    \n",
    "    system : A system setup message, typically used for global settings-related prompts.\n",
    "    human : A user input message.\n",
    "    ai : An AI response message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbf4014-5715-4285-9ee1-65cdcfd7beba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is the capital of United States of America?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chat_prompt.format(country=\"United States of America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc88b99d-c369-451b-a128-6d5100f2f05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a friendly AI assistant. Your name is Teddy.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Nice to meet you!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"You are a friendly AI assistant. Your name is {name}.\"),\n",
    "        (\"human\", \"Nice to meet you!\"),\n",
    "        (\"ai\", \"Hello! How can I assist you?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create chat messages\n",
    "messages = chat_template.format_messages(name=\"Teddy\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085870c-e9d5-46ad-9808-9b0462aba8a5",
   "metadata": {},
   "source": [
    "### MessagePlaceholder\n",
    "\n",
    "    LangChain also provides a MessagePlaceholder , which provides complete control over rendering messages during formatting.\n",
    "    This can be useful if you’re unsure which roles to use in a message prompt template or if you want to insert a list of messages during formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d09205-09e2-488c-b233-b87049dd927c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000022831E83B50>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='Summarize the conversation so far in {word_count} words.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"human\", \"Summarize the conversation so far in {word_count} words.\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a34d2d2-2d32-4cc7-80a5-bb3972350687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\n",
      "Human: Hello! I’m Teddy. Nice to meet you.\n",
      "AI: Nice to meet you! I look forward to working with you.\n",
      "Human: Summarize the conversation so far in 5 words.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"Hello! I’m Teddy. Nice to meet you.\"),\n",
    "        (\"ai\", \"Nice to meet you! I look forward to working with you.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec3c58-5d8c-4531-a8bd-792c377a04b5",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dde3d1-dcd2-4b9e-84b2-63874ec5ba7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PydanticOutputParser \n",
    "    The PydanticOutputParser is a class that helps transform the output of a language model into structured information. \n",
    "    This class can provide the information you need in a clear and organized form instead of a simple text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2b4452-4a79-4d11-9865-7a544ad8d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "LLM = ChatGroq(name='llama-3.2-3b-preview', api_key=os.environ.get('GROQ_API_KEY'), temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe8b5b3-97e5-48df-a4a0-63b0ebaaa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_conversation = \"\"\"\n",
    "From: John (John@bikecorporation.me)\n",
    "To: Kim (Kim@teddyinternational.me)\n",
    "Subject: “ZENESIS” bike distribution cooperation and meeting schedule proposal\n",
    "Dear Mr. Kim,\n",
    "\n",
    "I am John, Senior Executive Director at Bike Corporation. I recently learned about your new bicycle model, \"ZENESIS,\" through your press release. Bike Corporation is a company that leads innovation and quality in the field of bicycle manufacturing and distribution, with long-time experience and expertise in this field.\n",
    "\n",
    "We would like to request a detailed brochure for the ZENESIS model. In particular, we need information on technical specifications, battery performance, and design aspects. This information will help us further refine our proposed distribution strategy and marketing plan.\n",
    "\n",
    "Additionally, to discuss the possibilities for collaboration in more detail, I propose a meeting next Tuesday, January 15th, at 10:00 AM. Would it be possible to meet at your office to have this discussion?\n",
    "\n",
    "Thank you.\n",
    "\n",
    "Best regards,\n",
    "John\n",
    "Senior Executive Director\n",
    "Bike Corporation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f7261d-71a8-4939-9c5f-98538bb40f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template('''Please extract the information provided in the following email. \\n\\n {email} ''')\n",
    "chain = prompt | LLM | StrOutputParser()\n",
    "answer = chain.stream({'email':email_conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd4208d-2a8f-48d7-8729-508fc1689da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(response, return_output=False):\n",
    "    answer = ''\n",
    "    for token in response:\n",
    "        if isinstance(token , AIMessageChunk):\n",
    "            answer+= token.content + '\\n'\n",
    "            print(token.content, end=\"\", flush=True)\n",
    "        elif isinstance(token, str):\n",
    "            answer += token\n",
    "            print(token, end=\"\", flush=True)\n",
    "    if return_output:\n",
    "        return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff225f6d-0599-4699-b352-fec9e6eaf421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sender: John, Senior Executive Director at Bike Corporation (John@bikecorporation.me)\n",
      "* Recipient: Kim at Teddy International (Kim@teddyinternational.me)\n",
      "* Subject: \"ZENESIS\" bike distribution cooperation and meeting schedule proposal\n",
      "* Bike Corporation is interested in the \"ZENESIS\" bicycle model from Teddy International.\n",
      "* John has requested a detailed brochure for the ZENESIS model, including technical specifications, battery performance, and design aspects.\n",
      "* The requested information will help Bike Corporation refine their distribution strategy and marketing plan for the ZENESIS model.\n",
      "* John has proposed a meeting on Tuesday, January 15th, at 10:00 AM to discuss collaboration possibilities in detail.\n",
      "* John has suggested that the meeting take place at Teddy International's office."
     ]
    }
   ],
   "source": [
    "output = stream_response(answer, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6d3f9b-48af-47c3-ba31-eb2e10a1a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "class EmailExtract(BaseModel):\n",
    "    ''' Schema Class for Extracting Usefull information from provided email'''\n",
    "    person: str = Field(description=\"The sender of the email\")\n",
    "    email: str = Field(description=\"The email address of the sender\")\n",
    "    subject: str = Field(description=\"The subject of the email\")\n",
    "    summary: str = Field(description=\"A summary of the email content\")\n",
    "    date: str = Field(description=\"The meeting date and time mentioned in the email content\")\n",
    "\n",
    "email_parser = PydanticOutputParser(pydantic_object=EmailExtract)\n",
    "chain_pydantic = prompt | LLM | email_parser\n",
    "answer_pydantic = chain_pydantic.stream({\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"Extract the main content of the email.\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db571e5-47b8-4aa9-be9c-2d48d5edd137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Schema Class for Extracting Usefull information from provided email\", \"properties\": {\"person\": {\"description\": \"The sender of the email\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"The email address of the sender\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"The subject of the email\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"A summary of the email content\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"The meeting date and time mentioned in the email content\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(email_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f5801c-6f91-4a94-9dfe-2865510a731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. \n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EMAIL CONVERSATION:\n",
    "{email_conversation}\n",
    "\n",
    "FORMAT:\n",
    "{format}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add partial formatting of PydanticOutputParser to format\n",
    "prompt = prompt.partial(format=email_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077cec09-1ada-411b-aeb2-1630bf5b5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "073f973d-1a4d-4eff-a412-e123abccb6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON instance based on the provided email:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"person\": \"John\",\n",
      "  \"email\": \"John@bikecorporation.me\",\n",
      "  \"subject\": \"ZENESIS bike distribution cooperation and meeting schedule proposal\",\n",
      "  \"summary\": \"John, Senior Executive Director at Bike Corporation, is interested in the new bicycle model 'ZENESIS' from Teddy International. He requests a detailed brochure with technical specifications, battery performance, and design aspects to refine the distribution strategy and marketing plan. He proposes a meeting on January 15th at 10:00 AM at Teddy International's office.\",\n",
      "  \"date\": \"January 15th, 10:00 AM\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "# Execute the chain and print the result.\n",
    "response = chain.stream(\n",
    "    {\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"Extract the main content of the email.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# The result is provided in JSON format.\n",
    "output = stream_response(response, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e70fbfa1-b8a8-49ec-9ff2-200730c66fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: Here\n is\n the\n JSON\n instance\n based\n on\n the\n provided\n email\n:\n\n\n\n\n``\n`\njson\n\n\n{\n\n\n \n \"\nperson\n\":\n \"\nJohn\n\",\n\n\n \n \"\nemail\n\":\n \"\nJohn\n@\nb\nike\ncor\npor\nation\n.\nme\n\",\n\n\n \n \"\nsubject\n\":\n \"\nZ\nEN\nES\nIS\n bike\n distribution\n cooperation\n and\n meeting\n schedule\n proposal\n\",\n\n\n \n \"\nsummary\n\":\n \"\nJohn\n,\n Senior\n Executive\n Director\n at\n B\nike\n Corporation\n,\n is\n interested\n in\n the\n new\n bicy\ncle\n model\n '\nZ\nEN\nES\nIS\n'\n from\n Ted\ndy\n International\n.\n He\n requests\n a\n detailed\n bro\nch\nure\n with\n technical\n specific\nations\n,\n battery\n performance\n,\n and\n design\n aspects\n to\n ref\nine\n the\n distribution\n strategy\n and\n marketing\n plan\n.\n He\n propos\nes\n a\n meeting\n on\n January\n \n1\n5\nth\n at\n \n1\n0\n:\n0\n0\n AM\n at\n Ted\ndy\n International\n'\ns\n office\n.\",\n\n\n \n \"\ndate\n\":\n \"\nJan\nuary\n \n1\n5\nth\n,\n \n1\n0\n:\n0\n0\n AM\n\"\n\n\n}\n\n\n``\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:83\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\utils\\json.py:144\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string, parser)\u001b[0m\n\u001b[0;32m    143\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\utils\\json.py:160\u001b[0m, in \u001b[0;36m_parse_json\u001b[1;34m(json_str, parser)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\utils\\json.py:118\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[1;34m(s, strict)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m structured_output \u001b[38;5;241m=\u001b[39m \u001b[43memail_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(structured_output)\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:83\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:97\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m        The parsed JSON object.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:72\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partial:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:67\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_obj(json_object)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Code-Stuff\\envs\\langllm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:86\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     85\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Invalid json output: Here\n is\n the\n JSON\n instance\n based\n on\n the\n provided\n email\n:\n\n\n\n\n``\n`\njson\n\n\n{\n\n\n \n \"\nperson\n\":\n \"\nJohn\n\",\n\n\n \n \"\nemail\n\":\n \"\nJohn\n@\nb\nike\ncor\npor\nation\n.\nme\n\",\n\n\n \n \"\nsubject\n\":\n \"\nZ\nEN\nES\nIS\n bike\n distribution\n cooperation\n and\n meeting\n schedule\n proposal\n\",\n\n\n \n \"\nsummary\n\":\n \"\nJohn\n,\n Senior\n Executive\n Director\n at\n B\nike\n Corporation\n,\n is\n interested\n in\n the\n new\n bicy\ncle\n model\n '\nZ\nEN\nES\nIS\n'\n from\n Ted\ndy\n International\n.\n He\n requests\n a\n detailed\n bro\nch\nure\n with\n technical\n specific\nations\n,\n battery\n performance\n,\n and\n design\n aspects\n to\n ref\nine\n the\n distribution\n strategy\n and\n marketing\n plan\n.\n He\n propos\nes\n a\n meeting\n on\n January\n \n1\n5\nth\n at\n \n1\n0\n:\n0\n0\n AM\n at\n Ted\ndy\n International\n'\ns\n office\n.\",\n\n\n \n \"\ndate\n\":\n \"\nJan\nuary\n \n1\n5\nth\n,\n \n1\n0\n:\n0\n0\n AM\n\"\n\n\n}\n\n\n``\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "structured_output = email_parser.parse(output)\n",
    "print(structured_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711f759-1628-486f-94c8-84039e167411",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pydantic = stream_response(answer_pydantic, return_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538a9bc-8f95-48b0-aaa8-b0052cb15d0f",
   "metadata": {},
   "source": [
    "#### One thing to note is that the .with_structured_output() function does not support the stream() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d850a58-d8fb-4828-86eb-1811f469f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structured = LLM.with_structured_output(EmailExtract)\n",
    "answer = llm_with_structured.invoke(email_conversation)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e65fd-8dab-48c6-9582-f0bffc36b2ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CommaSeparatedListOutputParser \n",
    "\n",
    "    The CommaSeparatedListOutputParser is a specialized output parser in LangChain designed for generating structured outputs in the form of comma-separated lists.\n",
    "    \n",
    "    It simplifies the process of extracting and presenting data in a clear and concise list format, \n",
    "    making it particularly useful for organizing information such as data points, names, items, or other structured values. \n",
    "    By leveraging this parser, users can enhance data clarity, ensure consistent formatting, and improve workflow efficiency, especially in applications where structured outputs are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97396cf6-dbeb-4d52-aa14-13d8270de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "format_intructions = CommaSeparatedListOutputParser().get_format_instructions()\n",
    "format_intructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b80a0a-fb3a-415a-bc75-2b87a5be32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cs = PromptTemplate(template=\"List 7 {subject}. \\n\\n {{format_instructions}}\",\n",
    "                           input_variables=['subject'],\n",
    "                           partial_variables={'format_instructions' : format_intructions})\n",
    "\n",
    "prompt_cs.invoke({'subject' : 'famous Things to explore in Delhi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019e2b5-1ba5-4efe-af4e-f4c72c60a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4f6e3-2e90-42ea-82ef-e363f268e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_cs = prompt_cs | LLM | CommaSeparatedListOutputParser()\n",
    "output_cs = chain_cs.invoke({'subject' : 'famous places names to explore in Delhi'})\n",
    "output_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17445461-25ff-4b2d-959f-748e058ba686",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082d091-54ce-46b9-af95-782b7b3e5515",
   "metadata": {},
   "source": [
    "### StructuredOutputParser\n",
    "    Valuable tool for formatting Large Language Model (LLM) responses into dictionary structures, enabling the return of multiple fields as key/value pairs. \n",
    "    While Pydantic and JSON parsers offer robust capabilities, the StructuredOutputParser is particularly effective for less powerful models, such as local models with fewer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff347e57-c9fc-4a8c-8eeb-efc409c0a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "response_schema = [ ResponseSchema(name = \"answer\", description = \"Answer for the User's Query\"),\n",
    "                   ResponseSchema(name = \"source\", description = \"source from which the answer is inspired or collected from, it should be a Website URL\")]\n",
    "\n",
    "parser_ = StructuredOutputParser.from_response_schemas(response_schemas=response_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360687f7-7552-41cb-b75e-d72bf27bbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instruction = parser_.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4652ea-ceb6-45af-b22c-e904eea80913",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ = PromptTemplate(template='Answer the users query with the best answer possible. \\n\\n {query} \\n\\n {format_ins}',\n",
    "                        input_variables=['query'],\n",
    "                        partial_variables={'format_ins' : format_instruction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b84ac-23c8-41cf-ab9a-ab2a7c7fa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69b3f0-1126-4c70-b9a8-cfeda5de6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_ = prompt_ | LLM | parser_\n",
    "chain_.invoke({\"query\":\"What is the GDP of India?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd5422-2b74-4211-a6d0-0bc0da48ed0e",
   "metadata": {},
   "source": [
    "### JsonOutputParser\n",
    "\n",
    "    JsonOutputParser is a tool that allows users to specify the desired JSON schema. \n",
    "    It is designed to enable an LLM(Large Language Model) to query data and return results in JSON format that adheres to the specified schema. \n",
    "    To ensure that the LLM processes data accurately and efficiently, generating JSON in the desired format, the model must have sufficient capacity (e.g., intelligence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aaa76-155f-4b75-88dc-82cb8974ed63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
